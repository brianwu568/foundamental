# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-5-nano-2025-08-07\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-5-nano-2025-08-07\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Ultra-cheap nano model for evaluations\nclient<llm> GPTNano {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-5-nano-2025-08-07\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Alternative: Use GPT-3.5 Turbo as a cheaper eval model\nclient<llm> GPT35Turbo {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-3.5-turbo\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n\nclient<llm> OllamaLocal {\n  provider ollama\n  options {\n    model \"llama3\"\n    base_url \"http://localhost:11434\"\n  }\n}\n\nclient<llm> OllamaLlama3_1 {\n  provider ollama\n  options {\n    model \"llama3.1\"\n    base_url \"http://localhost:11434\"\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.213.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "llm_seo.baml": "class Answer {\n  name string\n  why string\n}\n\nclass RankingResult {\n  answers Answer[]\n}\n\n// Hallucination Filter: Source citation class\nclass Source {\n  url string\n  title string?\n  description string?\n}\n\n// Hallucination Filter: Answer with sources and confidence\nclass AnswerWithSources {\n  name string\n  why string\n  sources Source[]\n  confidence float @description(\"Confidence score from 0.0 to 1.0\")\n}\n\nclass RankingResultWithSources {\n  answers AnswerWithSources[]\n}\n\nenum Sentiment {\n  Positive\n  Neutral\n  Negative\n}\n\nclass SentimentResult {\n  sentiment Sentiment\n  confidence float\n}\n\nfunction RankEntities(query: string, k: int) -> RankingResult {\n  client \"openai/gpt-5-nano-2025-08-07\"\n\n  prompt #\"\n    You are a rankings engine. Given a user query, return the top-K entities\n    that best answer the query. Return STRICT JSON that conforms to the\n    output schema.\n\n    Query: {{ query }}\n    TopK: {{ k }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction RankEntitiesOpenAI(query: string, k: int) -> RankingResult {\n  client CustomGPT4oMini\n\n  prompt #\"\n    You are a rankings engine. Given a user query, return the top-K entities\n    that best answer the query. Return STRICT JSON that conforms to the\n    output schema.\n\n    Query: {{ query }}\n    TopK: {{ k }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction RankEntitiesOllama(query: string, k: int) -> RankingResult {\n  client OllamaLocal\n\n  prompt #\"\n    You are a rankings engine. Given a user query, return the top-K entities\n    that best answer the query. Return STRICT JSON that conforms to the\n    output schema.\n\n    Query: {{ query }}\n    TopK: {{ k }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction BrandSentiment(brand: string, passage: string) -> SentimentResult {\n  client \"openai/gpt-5-nano-2025-08-07\"\n\n  prompt #\"\n    Classify sentiment toward the brand in the passage.\n    Return STRICT JSON matching the schema.\n\n    Brand: {{ brand }}\n    Passage:\n    {{ passage }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Hallucination Filter: OpenAI ranking with sources and confidence\nfunction RankEntitiesWithSourcesOpenAI(query: string, k: int) -> RankingResultWithSources {\n  client CustomGPT4oMini\n\n  prompt #\"\n    You are a rankings engine with source attribution capabilities.\n    \n    Given a user query, return the top-K entities that best answer the query.\n    \n    IMPORTANT REQUIREMENTS:\n    1. For EACH entity, provide at least 1-3 credible source URLs that support why this entity is relevant\n    2. Include a confidence score (0.0 to 1.0) indicating how confident you are in this ranking\n    3. Only include entities you can support with real, verifiable sources\n    4. DO NOT make up URLs - if you cannot find a credible source, use a lower confidence score\n    \n    Return STRICT JSON that conforms to the output schema.\n\n    Query: {{ query }}\n    TopK: {{ k }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Hallucination Filter: Ollama ranking with sources and confidence\nfunction RankEntitiesWithSourcesOllama(query: string, k: int) -> RankingResultWithSources {\n  client OllamaLocal\n\n  prompt #\"\n    You are a rankings engine with source attribution capabilities.\n    \n    Given a user query, return the top-K entities that best answer the query.\n    \n    IMPORTANT REQUIREMENTS:\n    1. For EACH entity, provide at least 1-3 credible source URLs that support why this entity is relevant\n    2. Include a confidence score (0.0 to 1.0) indicating how confident you are in this ranking\n    3. Only include entities you can support with real, verifiable sources\n    4. DO NOT make up URLs - if you cannot find a credible source, use a lower confidence score\n    \n    Return STRICT JSON that conforms to the output schema.\n\n    Query: {{ query }}\n    TopK: {{ k }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest sample_rank {\n  functions [RankEntities]\n  args {\n    query \"best LLM providers\"\n    k 3\n  }\n}\n\ntest sample_sentiment {\n  functions [BrandSentiment]\n  args {\n    brand \"AcmeCo\"\n    passage #\"\n      People love AcmeCo's support, but the app is buggy.\n    \"#\n  }\n}\n\n// ============================================================================\n// LLM-as-a-Judge Evaluation Functions\n// ============================================================================\n\n// Result of brand matching evaluation\nclass BrandMatchResult {\n  is_match bool @description(\"Whether the text refers to the target brand\")\n  confidence float @description(\"Confidence score from 0.0 to 1.0\")\n  matched_alias string? @description(\"The specific alias or variation that was matched, if any\")\n  reasoning string @description(\"Brief explanation of why this is or isn't a match\")\n}\n\n// Result of evaluating multiple brands against a text\nclass BrandMatchBatchResult {\n  matches BrandMatch[]\n}\n\nclass BrandMatch {\n  brand_name string @description(\"The brand being evaluated\")\n  is_match bool\n  confidence float\n  matched_text string? @description(\"The specific text that matched the brand\")\n  reasoning string\n}\n\n// Evaluation result for comparing expected vs actual output\nclass EvalResult {\n  passed bool @description(\"Whether the evaluation passed\")\n  score float @description(\"Score from 0.0 to 1.0\")\n  feedback string @description(\"Detailed feedback on the evaluation\")\n  issues string[] @description(\"List of specific issues found, if any\")\n}\n\n// Function to check if a text mentions a specific brand (using cheap model)\nfunction EvalBrandMatch(text: string, brand_name: string, brand_aliases: string[]) -> BrandMatchResult {\n  client CustomGPT4oMini\n  \n  prompt #\"\n    You are an evaluation judge for brand mention detection.\n    \n    Determine if the given text refers to the target brand. Consider:\n    - Exact name matches\n    - Known aliases and variations\n    - Partial matches (e.g., \"OpenAI's product\" mentions \"OpenAI\")\n    - Common misspellings or abbreviations\n    - Context clues that clearly indicate the brand\n    \n    Target Brand: {{ brand_name }}\n    Known Aliases: {{ brand_aliases }}\n    \n    Text to evaluate:\n    {{ text }}\n    \n    Return your evaluation as JSON matching the schema.\n    Be generous with partial matches but confident about exact matches.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Function to evaluate brand matches using Ollama (free local model)\nfunction EvalBrandMatchOllama(text: string, brand_name: string, brand_aliases: string[]) -> BrandMatchResult {\n  client OllamaLocal\n  \n  prompt #\"\n    You are an evaluation judge for brand mention detection.\n    \n    Determine if the given text refers to the target brand. Consider:\n    - Exact name matches\n    - Known aliases and variations\n    - Partial matches (e.g., \"OpenAI's product\" mentions \"OpenAI\")\n    - Common misspellings or abbreviations\n    - Context clues that clearly indicate the brand\n    \n    Target Brand: {{ brand_name }}\n    Known Aliases: {{ brand_aliases }}\n    \n    Text to evaluate:\n    {{ text }}\n    \n    Return your evaluation as JSON matching the schema.\n    Be generous with partial matches but confident about exact matches.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Batch evaluation of multiple brands against a single text\nfunction EvalBrandMatchBatch(text: string, brands: string[]) -> BrandMatchBatchResult {\n  client CustomGPT4oMini\n  \n  prompt #\"\n    You are an evaluation judge for brand mention detection.\n    \n    Given a text, determine which of the provided brands are mentioned.\n    Consider exact matches, partial matches, abbreviations, and context clues.\n    \n    Brands to check: {{ brands }}\n    \n    Text to evaluate:\n    {{ text }}\n    \n    For each brand, provide:\n    - Whether it's mentioned (is_match)\n    - Confidence score (0.0 to 1.0)\n    - The specific text that matched (if any)\n    - Brief reasoning\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// General evaluation function for comparing expected vs actual outputs\nfunction EvalOutput(expected: string, actual: string, criteria: string) -> EvalResult {\n  client CustomGPT4oMini\n  \n  prompt #\"\n    You are an evaluation judge comparing expected vs actual outputs.\n    \n    Evaluation Criteria: {{ criteria }}\n    \n    Expected Output:\n    {{ expected }}\n    \n    Actual Output:\n    {{ actual }}\n    \n    Evaluate how well the actual output matches the expected output based on the criteria.\n    Consider semantic similarity, not just exact string matching.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Evaluation function using Ollama for free local evaluation\nfunction EvalOutputOllama(expected: string, actual: string, criteria: string) -> EvalResult {\n  client OllamaLocal\n  \n  prompt #\"\n    You are an evaluation judge comparing expected vs actual outputs.\n    \n    Evaluation Criteria: {{ criteria }}\n    \n    Expected Output:\n    {{ expected }}\n    \n    Actual Output:\n    {{ actual }}\n    \n    Evaluate how well the actual output matches the expected output based on the criteria.\n    Consider semantic similarity, not just exact string matching.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-5-nano-2025-08-07\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return _file_map